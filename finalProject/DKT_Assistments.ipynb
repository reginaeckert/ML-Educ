{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE \n",
    "from IPython.core.debugger import set_trace\n",
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response_df = pd.read_csv('correct.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "skill_df = pd.read_csv('skill.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "assistment_df = pd.read_csv('assistment_id.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "skill_dict = {}\n",
    "with open('skill_dict.json', 'r', encoding='utf-8') as f:\n",
    "    loaded = json.load(f)\n",
    "    for k, v in loaded.items():\n",
    "        skill_dict[k] = int(v)\n",
    "\n",
    "skill_num = len(skill_dict) + 1 # including 0\n",
    "#skill_num=len(np.unique(skill_df.iloc[:,2:51]))\n",
    "#Need to deal with having unequal number of problems for each student. Is this allowable in DKT?\n",
    "skill_df100=skill_df[skill_df.num_resp>100].copy()\n",
    "assistment_df100=assistment_df[assistment_df.num_resp>100].copy()\n",
    "response_df100=response_df[response_df.num_resp>100].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create assistment encoding\n",
    "assistment_re_df100=assistment_df100.copy()\n",
    "assistment_enc_mat=assistment_df100.iloc[:,2:103].values.astype(int)\n",
    "assistment_enc=np.reshape(assistment_enc_mat,assistment_enc_mat.shape[0]*assistment_enc_mat.shape[1])\n",
    "\n",
    "[assistment_enc2,assistment_enc2_counts]=np.unique(assistment_enc,return_counts=True) #Unique assistment in the set, and how often it occurs\n",
    "num_assist=len(assistment_enc2) #Number of unique assistments\n",
    "assistment_label=np.arange(num_assist) #Label 0:num_assist\n",
    "\n",
    "skill_enc_mat=skill_df100.iloc[:,2:103].values.astype(int)\n",
    "skill_enc2=np.empty((num_assist,1),dtype=int)\n",
    "\n",
    "for ii in range(num_assist):\n",
    "    indexArray=assistment_enc_mat==assistment_enc2[ii]\n",
    "    assistment_enc_mat[indexArray]=ii #0 to num_assist encoding of assistment ids\n",
    "    skill_enc=skill_enc_mat[indexArray] #All the skills associated with that assistment id (number 0 to num_skill)\n",
    "    skill_enc2[ii]=skill_enc[0] #Choose the first one\n",
    "    #Associate assistment with skill\n",
    "        \n",
    "assistment_re_df100.iloc[:,2:103]=assistment_enc_mat #Copy over to data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot(skill_matrix, vocab_size):\n",
    "    '''\n",
    "    params:\n",
    "        skill_matrix: 2-D matrix (student, skills)\n",
    "        vocal_size: size of the vocabulary\n",
    "    returns:\n",
    "        a ndarray with a shape like (student, sequence_len, vocab_size)\n",
    "    '''\n",
    "    seq_len = skill_matrix.shape[1] #Number of exercises (sequence length)\n",
    "    #Initialize result (student, sequence, one-hot skill) to zeros\n",
    "    result = np.zeros((skill_matrix.shape[0], seq_len, vocab_size)) \n",
    "    #For each student\n",
    "    for i in range(skill_matrix.shape[0]):\n",
    "        #Select the student, all sequences, and the related skill; set to 1\n",
    "        result[i, np.arange(seq_len), skill_matrix[i]] = 1.\n",
    "    return result\n",
    "\n",
    "def dkt_one_hot(skill_matrix, response_matrix, vocab_size):\n",
    "    #Number of exercises/skills\n",
    "    seq_len = skill_matrix.shape[1]\n",
    "    #Initialize output (student, sequence, 2 * vocab size) to zeros\n",
    "    skill_response_array = np.zeros((skill_matrix.shape[0], seq_len, 2 * vocab_size))\n",
    "    #For each student\n",
    "    for i in range(skill_matrix.shape[0]):\n",
    "        #Set to 1 the (student, all sequences, skill location + [0 1] if correct and + [1 0] if incorrect)\n",
    "        skill_response_array[i, np.arange(seq_len), 2 * skill_matrix[i] + response_matrix[i]] = 1.\n",
    "    return skill_response_array\n",
    "\n",
    "\n",
    "#Function to preprocess the data\n",
    "def preprocess(skill_df, response_df, skill_num):\n",
    "    skill_matrix = skill_df.iloc[:, 2:].values.astype(int) #Select values (excluding first column, which is index)\n",
    "    response_array = response_df.iloc[:, 2:].values.astype(int)\n",
    "    #Get the one-hots associated with each (student, sequence, skill one-hot)\n",
    "    skill_array = one_hot(skill_matrix, skill_num)\n",
    "    #Get the one-hots associated with (student, sequence, response one-hot)\n",
    "    #and (student, sequence, skill one-hot)\n",
    "    #skill_response_array, masking_array = dkt_one_hot(skill_matrix, response_array, skill_num)\n",
    "    skill_response_array = dkt_one_hot(skill_matrix, response_array, skill_num)\n",
    "    return skill_array, response_array, skill_response_array\n",
    "    \n",
    "\n",
    "assist_array, response_array, assist_response_array = preprocess(assistment_re_df100.iloc[:,0:103], response_df100.iloc[:,0:103], num_assist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, LSTM, TimeDistributed, Lambda, multiply\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#Function to set up skill to skill model (input skills, output skill prediction)\n",
    "def build_skill2skill_model(input_shape, lstm_dim=32, dropout=0.0):\n",
    "    input = Input(shape=input_shape, name='input_skills')\n",
    "    lstm = LSTM(lstm_dim, \n",
    "                return_sequences=True, \n",
    "                dropout=dropout,\n",
    "                name='lstm_layer')(input)\n",
    "    output = TimeDistributed(Dense(input_shape[-1], activation='softmax'), name='probability')(lstm)\n",
    "    model = Model(inputs=[input], outputs=[output])\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def reduce_dim(x):\n",
    "    x = K.max(x, axis=-1, keepdims=True)\n",
    "    return x\n",
    "\n",
    "#Skill and response\n",
    "def build_dkt_model(input_shape, lstm_dim=32, dropout=0.0):\n",
    "    input_skills = Input(shape=input_shape, name='input_skills') #A, input, [skill(identified) correctness]\n",
    "    #LSTM hidden layer, processing input_skills\n",
    "    lstm = LSTM(lstm_dim, \n",
    "                return_sequences=True, \n",
    "                dropout=dropout,\n",
    "                name='lstm_layer')(input_skills)\n",
    "    \n",
    "    #Output layer, acting on outputs of LSTM\n",
    "    #Probability of each skill being correct upon the next question\n",
    "    dense = TimeDistributed(Dense(int(input_shape[-1]/2), activation='sigmoid'), name='probability_for_each')(lstm)\n",
    " \n",
    "    #b, input, [actual next skill (identified)]\n",
    "    skill_next = Input(shape=(input_shape[0], int(input_shape[1]/2)), name='next_skill_tested')\n",
    "    #Select the actual next skill's probability\n",
    "    merged = multiply([dense, skill_next], name='multiply')\n",
    "    #Get only that result --> this is the output\n",
    "    reduced = Lambda(reduce_dim, output_shape=(input_shape[0], 1), name='reduce_dim')(merged)\n",
    "    \n",
    "    #Optimize using Adam\n",
    "    model = Model(inputs=[input_skills, skill_next], outputs=[reduced])\n",
    "    model2 = Model(inputs=[input_skills], outputs=[dense])\n",
    "    \n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model, model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('dkt')\n",
    "dkt_model, dkt_model2 = build_dkt_model((100, 2 * num_assist), lstm_dim=64)\n",
    "\n",
    "dkt_model.fit([assist_response_array[:, 0:-1], assist_array[:, 1:]],\n",
    "              response_array[:, 1:, np.newaxis],\n",
    "              epochs=20, \n",
    "              batch_size=32, \n",
    "              shuffle=True,\n",
    "              validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decrease amount of data for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select most popular assistments for each skill \n",
    "#106 skills, 9500 or so assistments\n",
    "#20 assistments/skill\n",
    "skill_un=np.unique(skill_enc2) #Unique skills in the list (labeled 0 to num_skill)\n",
    "skill_num=len(skill_un) #Number of unique ones\n",
    "top_num=30 #Number to select for each skill (unless there are less than this available)\n",
    "\n",
    "countA=np.zeros(skill_num)\n",
    "\n",
    "assist_index_test=list()\n",
    "\n",
    "#Loop through\n",
    "for ii in range(skill_num):\n",
    "    \n",
    "    #Get assistments assoc with each unique skill\n",
    "    assist_skill=np.arange(num_assist)[skill_enc2[:,0]==skill_un[ii]]\n",
    "    assist_skill_count=assistment_enc2_counts[assist_skill] #Select the counts for those skills\n",
    "    \n",
    "    #Sort\n",
    "    #sort_ind=np.argsort(assist_skill_count) #Index for sorted skills\n",
    "    top_num_actual=np.fmin(top_num,len(assist_skill_count)) #Select the smaller - top_num, or length\n",
    "    top_ind=assist_skill_count.argsort()[::-1][:top_num_actual] #Get the biggest n\n",
    "        \n",
    "    \n",
    "    #Get top ones\n",
    "    assist_top=assist_skill[top_ind] #Get labels (labeled 0 to num_assist) associated with those assistments\n",
    "    countA[ii]=top_num_actual\n",
    "    #Store it\n",
    "    \n",
    "    assist_index_test=np.append(assist_index_test,assist_top)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assist_index2=assist_index_test.astype('int')#np.arange(num_assist)[assist_index[:,0]] #Index in int labels, not logical index\n",
    "num_assist2=len(assist_index2)\n",
    "num_assist2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create influence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_matrix=np.zeros((num_assist2,num_assist))\n",
    "seq_len=100\n",
    "for i in range(num_assist2):\n",
    "    assist_response_array2 = np.zeros((1,seq_len,2 * num_assist)) #Set it up\n",
    "    assist_response_array2[0, np.arange(seq_len), 2 * assist_index2[i] + 1] = 1. #One-hot for that skill, + correct\n",
    "    #set_trace()\n",
    "    temp=dkt_model2.predict(assist_response_array2)\n",
    "    q_matrix[i,:]=temp[0,0,:]\n",
    "#Select the sub-matrices\n",
    "q_matrix_subset=q_matrix[:,assist_index2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(q_matrix_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_matrix_sum=np.sum(q_matrix_subset,axis=0)\n",
    "q_matrix_n=q_matrix_subset/np.tile(q_matrix_sum,[num_assist2, 1])\n",
    "plt.imshow(q_matrix_n)\n",
    "q_matrix_sym=np.add(q_matrix_n,q_matrix_n.transpose())/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-written spectral clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G=nx.from_numpy_matrix(q_matrix_sym)\n",
    "L1 = nx.linalg.laplacian_matrix(G)\n",
    "L1_nd=L1.toarray()\n",
    "S1 = nx.linalg.laplacian_spectrum(G)\n",
    "eigVal,eigVec = np.linalg.eig(L1_nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureLen=10;\n",
    "featureVec=eigVec[:,0:featureLen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NCr = 64\n",
    "sse = np.zeros(NCr)\n",
    "for NC in range(NCr): # number of clusters for K-means\n",
    "    kmeanse = cluster.KMeans(n_clusters=(NC+1)).fit(featureVec)\n",
    "    labels = kmeanse.labels_\n",
    "    centers = kmeanse.cluster_centers_\n",
    "    for i in range(NC+1):\n",
    "        sse[NC] += np.sum((featureVec[np.where(labels==i)]-centers[i])**2)\n",
    "        \n",
    "plt.plot(np.arange(NCr)+1,sse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NC = 32# number of clusters\n",
    "\n",
    "kmeansdkt = cluster.KMeans(n_clusters=NC).fit(featureVec)\n",
    "labelsdkt = kmeansdkt.labels_\n",
    "\n",
    "centers = kmeansdkt.cluster_centers_ # returns centers x dimensionality of space\n",
    "\n",
    "pd.Series(labelsdkt).value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spectral = cluster.SpectralClustering(n_clusters=32,affinity='precomputed')\n",
    "#temp=spectral.fit(q_matrix_n)#q_matrix_sym)#\n",
    "temp2=spectral.fit(q_matrix_sym)#\n",
    "#dkt_clusters=spectral.fit_predict(q_matrix_n)#q_matrix_n)\n",
    "dkt_clusters2=spectral.fit_predict(q_matrix_sym)#q_matrix_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(dkt_clusters2).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert dict to an array\n",
    "d2={value:key for key, value in skill_dict.items()}\n",
    "skill_list=list(range(107))\n",
    "for key, value in d2.items():\n",
    "    skill_list[int(key)]=value\n",
    "    \n",
    "skill_list=np.array(skill_list)\n",
    "\n",
    "#Select the skill labels that are actually used\n",
    "skill_enc3=skill_enc2[assist_index2]\n",
    "skill_list_enc2=skill_list[skill_enc2] #Convert to words\n",
    "skill_list_enc3=skill_list[skill_enc3[:,0]]\n",
    "assistment_enc3=assistment_enc2[assist_index2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureLen=10;\n",
    "featureVec=eigVec[:,0:featureLen]\n",
    "tsne=TSNE(perplexity=30) #Instantiate the TSNE model (can change params here)\n",
    "assist_tsne=tsne.fit_transform(featureVec.astype(float)) #Run tsne\n",
    "\n",
    "\n",
    "tsne_save=pd.DataFrame({'x': assist_tsne[:,0],\n",
    "  'y': assist_tsne[:,1],\n",
    "  'skill' : skill_list_enc3})\n",
    "tsne_save.to_csv('../d3-scatterplot/tsne_dkt_assist.tsv',sep='\\t',index=False,columns=['x','y','skill'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('../data/skills_dkt_fixed.npz', skills_dkt=skill_list_enc3)\n",
    "np.savez('../data/assist_dkt_fixed.npz', assist_dkt=assistment_enc3)\n",
    "np.savez('../data/labels_dkt_fixed.npz', labels_dkt=dkt_clusters2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('../data/labels_dkt_self.npz', labels_dkt=labelsdkt)\n",
    "np.savez('../data/vectors_dkt.npz', vectors_dkt=featureVec)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [pythonEnv]",
   "language": "python",
   "name": "Python [pythonEnv]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
