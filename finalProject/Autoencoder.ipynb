{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE \n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### LOAD DATA INSTEAD OF RUNNING THIS #############################################\n",
    "# Create \"body_skill.csv\": csv containging unique, non-nan bodies and associated skill\n",
    "\n",
    "# load pre-selected dataset\n",
    "df3 = pd.read_csv('../../data/df3.csv')\n",
    "\n",
    "# get unique problem text\n",
    "body = df3['body'].unique()\n",
    "body.shape\n",
    "\n",
    "# Confirm that, yes, some problems have the same text\n",
    "dfsub = df3[['problem_id','body']].drop_duplicates(subset='problem_id')\n",
    "dfbody = dfsub.groupby('body').count()\n",
    "# dfbody is how many different problem ids each body is associated with\n",
    "\n",
    "# Begin processing to get skills for each (I know this is inefficeint...)\n",
    "body_small = body\n",
    "\n",
    "body_skill = [] # list of all skills, in orders\n",
    "body_edited = []\n",
    "for problem in tqdm(body_small):\n",
    "    if isinstance(problem, str):\n",
    "        body_edited.append(problem)\n",
    "        skill = df3[df3['body'] == problem]['skill_name'].iloc[0] # get the first skill associated with the assistment\n",
    "        body_skill.append(skill)\n",
    "        \n",
    "# save above\n",
    "body_save=pd.DataFrame({'body': body_edited,\n",
    "  'skill_name': body_skill})\n",
    "body_save.to_csv('../../data/body_skill_v2.csv',sep='\\t',index=False,columns=['body','skill_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20123,)\n"
     ]
    }
   ],
   "source": [
    "###### LOAD DATA ########\n",
    "df = pd.read_csv('../../data/body_skill.csv', sep='\\t')\n",
    "body = df['body'].values\n",
    "skills = df['skill_name'].values\n",
    "print(body.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "body_small = body\n",
    "print(type(body_small))\n",
    "body_small.shape\n",
    "\n",
    "def removeHTML(x):\n",
    "    # Remove not useful characters and HTML text\n",
    "    x = re.sub('<.*?>', '', x)\n",
    "    x = re.sub('\\n', ' ', x)\n",
    "    x = re.sub('&nbsp;', ' ', x)\n",
    "    x = re.sub('\\?', '', x)\n",
    "    x = re.sub('\\\\.(\\\\s)', ' ', x)\n",
    "    x = re.sub(',', '', x)\n",
    "    x = re.sub(':', '', x)\n",
    "    x = re.sub(';', ' ', x)\n",
    "    x = re.sub('\\(', '', x)\n",
    "    x = re.sub('\\)', '', x)\n",
    "    return x\n",
    "\n",
    "# Make list of lists of words in each problem\n",
    "problems = []   # list of lists of words in each problem\n",
    "wordsAll = []   # list of all words (duplicate words listed more than once)\n",
    "wordsPerProblem = Counter() # count of how many different problems each word occurred in\n",
    "for problem in tqdm(body_small):\n",
    "    if isinstance(problem, str):\n",
    "        text = removeHTML(problem.lower()) # remove HTML text\n",
    "        words = text.split() # split into words at white space\n",
    "        wordsPerProblem += Counter(set(words))\n",
    "        wordsAll += words\n",
    "        problems.append(words)\n",
    "        #print(Counter(set(words)).most_common(), '\\n\\n')\n",
    "    \n",
    "print(len(wordsAll))\n",
    "print(wordsPerProblem.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(body[2200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(skills[2200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(removeHTML(body[2200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.where(tfidf[2200,:]>0))\n",
    "print(np.min(tfidf[2200,np.where(tfidf[2200,:]>0)]))\n",
    "print(np.asarray(wordDict)[np.where(tfidf[2200]>0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at the most common words in the dataset\n",
    "commonWords = list(np.array(Counter(wordsAll).most_common(50))[:,0])\n",
    "print(commonWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of problems:  20123\n",
      "['=', 'of', 'a', 'to', '2', 'number', 'in', '3', '-', '+', 'for', 'we', 'and', 'the', '1', 'on', 'is', '5', '6', '4', 'x', 'that', 'you', '7', 'answer', 'what', '8', 'this', 'below', 'by', 'your', 'equation', 'each', 'now', 'both', '10', 'value', 'from', 'find', 'do', 'write', 'if', 'following', 'points', 'as', 'students', '*', 'be', 'week', 'are', '9', 'sold', 'use', 'fraction', 'how', 'has', 'will', 'problem', 'day', 'y', 'percent', '12', 'plot', 'which', 'scored', 'at', 'shows', 'graph', 'one', 'first', 'line', 'type', 'numbers', 'nearest', 'original', 'between', 'order', 'solve', 'draw', 'he', 'with', '11', 'round', 'leaf', 'stem', 'expression', 'many', 'c', 'his', 'not', 'example', 'two', 'like', 'know', 'can', 'have', 'solution', 'b', 'an', 'miles', 'favorite', 'there', 'probability', 'show', 'last', '&radic', 'try', 'table', 'or', 'total', 'need', 'operations', '18', 'get', 'players', 'games', '13', 'years', '14', 'was', 'spaces', '16', '15', '&divide', '^', 'marble', 'length', 'all', 'form', 'simplify', 'must', '20', 'make', '0', 'exponent', 'again', 'into', 'smallest', 'blue', 'their', 'when', 'negative', 'side', 'green', 'it', 'fish', 'results', 'triangle', \"let's\", 'just', 'jar', 'per', 'bag', 'multiplication', 'right', 'so', 'cookies', 'factors', 'around', '24', '19', 'according', 'were', 'exponents', 'question', 'should', 'feet', '21', '22', 'race', 'ft', 'marbles', 'represents', 'using', 'â\\x96¡', 'terms', 'true', 'next', '17', 'wants', 'graphing', 'place', 'no', 'whole', 'decimal', 'going', 'chocolate', 'left', 'john', 'angle', 'sum', 'game', 'least', 'divide', 'restaurants', 'billy', 'same', 'red', 'convert', 'add', 'either', 'clothes', 'then', '32', 'french', 'hours', 'store', 'therefore', 'function', 'written', 'equal', 'x^3', 'w^5', '6x^2y^3', '-no', 'calculator-use', 'power', 'parentheses', \"-don't\", '6x2y3', '-put', 'second', 'median', 'subtract', '2x', 'slope', 'cm', 'cups', 'area', 'cookie', 'salary', '36', 'restaurant', 'cans', '30', '52', 'participated', 'winter', 'diagram', 'ball', 'makes', '40', 'denominator', 'cars', 'similar', 'shown', '&', '23', '-2', 'every', 'shelf', '-3', 'made', 'box', 'statement', '25', '50', 'enter', 'figure', 'player', 'basketball', 'equations', 'rectangular', 'greatest', 'y-intercept', '-1', 'sweaters', 'tea', 'necessary', '42', 'since', 'prism', 'sure', 'give', 'zero', 'these', 'than', 'lets', '-10', 'circle', 'team', 'return', 'constant', 'pick', 'food', '4x', 'term', 'took', 'fill', 'chinese', 'back', '-7', '35', 'way', 'd', 'glasses', 'polynomial', 'club', 'distribute', 'through', 'reduce', 'perform', 'needs', 'would', 'coach', 'name', 'take', '28', 'bars', 'sides', 'multiply', 'equality', 'signs', 'properties', 'equivalent', 'rate', 'class', 'year', 'after', '100', 'out', 'range', 'put', 'mean', 'inches', '-8', 'above', 'space', 'selected', 'outcomes', 'parenthesis', 'height', 'event', 'gets', 'number.', 'they', 'new', 'contains', 'student', 'saved', '-6', 'scatter', 'combine', 'ratio', 'notice', 'car', 'traveled', '/', 't', 'inequality', '-4', 'other', '27', '26', '&bull', 'answer.*', 'variable', 'ten', 'chip', 'mexican', '-5', 'annual', 'telephone', 'map', 'h', 'here', '60', 'looking', 'square', 'much', 'linear', 'y-3', 'equals', 'dogs', 'lines', '45', 'candies', 'p', 'speed', 'inside', 'fruit', 'width', 'positive', 'dollars', '34', 'hundredths', 'calculate', 'jason', 'city', 'standard', 'pets', '[', 'tenth.', 'soda', 'recycled', 'assuming', 'sharks', '-*no', 'needed', 'less', '29', 'about', 'convertibles', 'shoes', 'money', 'minutes', 'long', 'italian', 'picking', 'up', '38', 'chris', '33', 'who', 'hour', 'studied', 'brought', 'her', '44', '39', 'degree', 'lowest', 'did', 'used', 'together', 'also', 'division', 'skiing', 'largest', '3.14', 'venn', 'brownies', '1/y^3', 'asks', 'employment', '37', 'survey', 'look', 'want', 'orange', '-20', 'water', 'difference', '70', '55', 'gallons', 'sugar', 'quarts', 'spoons', 'determine', 'increase', '31', 'itself', 'all-star', '-9', 'mode', 'slope-intercept', '43', 'detergent', 'note', 'continues', 'but', '56', '2x3', 'without', 'tall', 'school', 'random', '41', 'shirts', 'measure', 'them', '49', 'highest', 'subtraction', '54', 'oatmeal', 'arranged', 'factor', '48', 'let', 'working', 'roots', 'days', '72', 'recorded', 'point', \"student's\", 'average', 'choice', '5x', 'pi', '3y2', 'list', 'tenths', 'amanda', '__1__', 'dividing', 'roll', 'possible', 'diameter', 'jeff', '2x^3/3y^2', '1/2', 'steve', 'fence', 'powers', 'correct', 'sports', 'lion', '6x', '__2__']\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary based on highedt average TFIDF\n",
    "N = len(problems)\n",
    "print('Total Number of problems: ',N)\n",
    "\n",
    "vs = 512 # vocab size\n",
    "dWordsAll = dict(Counter(wordsAll))\n",
    "dWordsProb = dict(wordsPerProblem)\n",
    "dDivide = {k: dWordsAll[k]/N*np.log(N/dWordsProb[k]) for k in dWordsProb.keys() & dWordsAll }\n",
    "c = Counter(dDivide)\n",
    "wordDict = list(np.array(c.most_common(vs))[:,0])\n",
    "print(wordDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "# tf_ij: number of times word i appears in problem j\n",
    "# df_i: number of problems containing word i\n",
    "# N: number of problems\n",
    "\n",
    "problems_small = problems\n",
    "N = float(len(problems))\n",
    "print('Number of problems: ',N)\n",
    "\n",
    "tfidf = []\n",
    "for problem in tqdm(problems_small):\n",
    "    c = Counter(problem)\n",
    "    tfidf_row = []\n",
    "    for word in wordDict:\n",
    "        tfidf_row.append(c[word]*np.log(N/wordsPerProblem[word]))\n",
    "    tfidf.append(np.array(tfidf_row))\n",
    "    \n",
    "tfidf = np.array(tfidf)\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.imshow(tfidf[::50,:])\n",
    "plt.colorbar()\n",
    "plt.clim((0,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run tsne\n",
    "tsne=TSNE(perplexity=30,verbose=2) #Instantiate the TSNE model (can change params here)\n",
    "tfidf_tsne=tsne.fit_transform(xviz) #Run tsne\n",
    "\n",
    "tsne_save=pd.DataFrame({'x': tfidf_tsne[:,0],\n",
    "  'y': tfidf_tsne[:,1],\n",
    "  'skill' : skills_viz})\n",
    "tsne_save.to_csv('../d3-scatterplot/tsne_body_bagofwords.tsv',sep='\\t',index=False,columns=['x','y','skill'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_tsne.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split bag of words data into training and testing set\n",
    "xtrain = []\n",
    "xtest =[]\n",
    "skills_train = []\n",
    "skills_test = []\n",
    "for i in np.arange(tfidf.shape[0]):\n",
    "    if np.mod(i,10) == 0:\n",
    "        xtest.append(tfidf[i,:])\n",
    "        skills_test.append(skills[i])\n",
    "    else:\n",
    "        xtrain.append(tfidf[i,:])\n",
    "        skills_train.append(skills[i])\n",
    "        \n",
    "xtrain = np.stack(xtrain, axis=0)\n",
    "xtest = np.stack(xtest, axis=0)\n",
    "skills_train = np.stack(skills_train, axis=0)\n",
    "skills_test = np.stack(skills_test, axis=0)\n",
    "print(xtrain.shape)\n",
    "print(xtest.shape)\n",
    "print(skills_train.shape)\n",
    "print(skills_test.shape)\n",
    "\n",
    "# Create visualization set (no nans)\n",
    "xviz = []\n",
    "skills_viz = []\n",
    "for i in np.arange(1,tfidf.shape[0]):\n",
    "    if isinstance(skills[i],str):\n",
    "        xviz.append(tfidf[i,:])\n",
    "        skills_viz.append(skills[i])\n",
    "        \n",
    "xviz = np.stack(xviz, axis=0)\n",
    "skills_viz = np.stack(skills_viz, axis=0)\n",
    "\n",
    "plt.hist(np.ndarray.flatten(tfidf), bins=50)\n",
    "plt.ylim([0, 100000])\n",
    "\n",
    "norm = 20 # normalization\n",
    "xtrain = xtrain/norm\n",
    "xtest = xtest/norm\n",
    "xviz = xviz/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 64  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "input_img = Input(shape=(vs,))\n",
    "encoded = Dense(256, activation='relu')(input_img)\n",
    "encoded = Dense(128, activation='relu')(encoded)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(128, activation='relu')(encoded)\n",
    "decoded = Dense(256, activation='relu')(decoded)\n",
    "decoded = Dense(vs, activation='sigmoid')(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layers of the autoencoder model\n",
    "decoder_layers = autoencoder.layers[-3](encoded_input)\n",
    "decoder_layers = autoencoder.layers[-2](decoder_layers)\n",
    "decoder_layers = autoencoder.layers[-1](decoder_layers)\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layers)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(xtrain, xtrain,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(xtest, xtest))\n",
    "\n",
    "# save models\n",
    "autoencoder.save('../../data/autoencoder_model.h5')\n",
    "decoder.save('../../data/decoder_model.h5')\n",
    "encoder.save('../../data/encoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = autoencoder.predict(tfidf/20)\n",
    "print(np.asarray(wordDict)[np.where(tfidf[2200,:]>.03)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded = encoder.predict(xviz)\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SAVE autoencoder output #\n",
    "print (xviz.shape)\n",
    "print (len(skills_viz))\n",
    "print (encoded.shape)\n",
    "np.savez('../../data/autoencoder_output.npz', xviz=xviz, skills_viz=skills_viz, encoded=encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test decoder\n",
    "auto = autoencoder.predict(xviz)\n",
    "decoded = decoder.predict(encoded)\n",
    "print(np.max(np.abs(auto[1,:]-decoded[1,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne=TSNE(perplexity=30,verbose=2) #Instantiate the TSNE model (can change params here)\n",
    "tfidf_tsne=tsne.fit_transform(encoded) #Run tsne\n",
    "tsne_save=pd.DataFrame({'x': tfidf_tsne[:,0],\n",
    "  'y': tfidf_tsne[:,1],\n",
    "  'skill' : skills_viz})\n",
    "#tsne_save=pd.DataFrame({'x': encoded[:,0],\n",
    "#  'y': encoded[:,1],\n",
    "#  'skill' : skills_viz})\n",
    "tsne_save.to_csv('../d3-scatterplot/tsne_bagofwords_autoencoder.tsv',sep='\\t',index=False,columns=['x','y','skill'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xpred = autoencoder.predict(tfidf/20)\n",
    "plt.figure(figsize=(14,8))\n",
    "n = 2200\n",
    "plt.plot(tfidf[n,:]/20)\n",
    "plt.plot(xpred[n,:], '--')\n",
    "#print(xpred[n,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read in data ##\n",
    "# xviz - tfidf encodings for all problems that have associated skills\n",
    "# skills_viz - skill associated with each problem (list)\n",
    "# encoded - autoencoder output for all problems in xviz\n",
    "data = np.load('../../data/autoencoder_output.npz')\n",
    "xviz = data['xviz']\n",
    "skills_viz = data['skills_viz']\n",
    "encoded = data['encoded']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "NC = 32 # number of clusters for K-means\n",
    "kmeanse = KMeans(n_clusters=NC).fit(encoded)\n",
    "kmeansb = KMeans(n_clusters=NC).fit(xviz) #bag of words encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelse = kmeanse.labels_\n",
    "labelsb = kmeansb.labels_\n",
    "print(pd.Series(skills_viz[np.where(labelse==11)]).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(skills_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unique skills\n",
    "skills_u = pd.Series(skills_viz).unique()\n",
    "print(skills_u.shape)\n",
    "\n",
    "# Which skills are in each cluster\n",
    "A = np.zeros((skills_u.shape[0], NC))\n",
    "B = np.zeros((skills_u.shape[0], NC))\n",
    "\n",
    "# how many of each skill are in each cluster\n",
    "A1 = np.zeros((skills_u.shape[0], NC))\n",
    "B1 = np.zeros((skills_u.shape[0], NC))\n",
    "\n",
    "for i in range(NC):\n",
    "    unqe = pd.Series(skills_viz[np.where(labelse==i)]).unique()\n",
    "    unqb = pd.Series(skills_viz[np.where(labelsb==i)]).unique()\n",
    "    for skill in unqe:\n",
    "        A[np.where(skills_u==skill), i] += 1;\n",
    "        A1[np.where(skills_u==skill), i] = pd.Series(skills_viz[np.where(labelse==i)]).value_counts()[skill]\n",
    "    for skill in unqb:\n",
    "        B[np.where(skills_u==skill), i] += 1;\n",
    "        B1[np.where(skills_u==skill), i] = pd.Series(skills_viz[np.where(labelsb==i)]).value_counts()[skill]\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(skills_viz[np.where(labelse==80)]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(A1)\n",
    "plt.xlabel('cluster')\n",
    "plt.ylabel('skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(B1)\n",
    "plt.xlabel('cluster')\n",
    "plt.ylabel('skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.sum(A, axis=1))\n",
    "plt.xlabel('Skill')\n",
    "plt.figure()\n",
    "plt.plot(np.sum(A, axis=0))\n",
    "plt.xlabel('Cluster')\n",
    "plt.figure()\n",
    "plt.plot(np.sum(A1, axis=0))\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of things per cluster')\n",
    "print('Each skill is in an average of', np.mean(np.sum(A, axis=1)), 'clusters')\n",
    "print('Each cluster has an average of', np.mean(np.sum(A, axis=0)), 'skills')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.sum(B, axis=1))\n",
    "plt.xlabel('Skill')\n",
    "plt.figure()\n",
    "plt.plot(np.sum(B, axis=0))\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of skills per cluster')\n",
    "plt.figure()\n",
    "plt.plot(np.sum(B1, axis=0))\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of problems per cluster')\n",
    "print('Each skill is in an average of', np.mean(np.sum(B, axis=1)), 'clusters')\n",
    "print('Each cluster has an average of', np.mean(np.sum(B, axis=0)), 'skills')\n",
    "\n",
    "s = np.sum(B1, axis=0)\n",
    "print(s[23])\n",
    "print(s[21]/np.sum(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xviz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's look at the cluster centers\n",
    "centers = kmeanse.cluster_centers_ # returns centers x dimensionality of space\n",
    "decoded_centers = decoder.predict(centers)\n",
    "\n",
    "s = 30;\n",
    "\n",
    "c = decoded_centers[s,:]\n",
    "b = np.zeros(c.shape)\n",
    "b = np.where(c>np.mean(c))\n",
    "w = [wordDict[i] for i in np.ndarray.tolist(b[0])]\n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(pd.Series(skills_viz[np.where(labelse==s)]).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NCr = 64\n",
    "sse = np.zeros(NCr)\n",
    "for NC in tqdm(range(NCr)): # number of clusters for K-means\n",
    "    kmeanse = KMeans(n_clusters=(NC+1)).fit(encoded)\n",
    "    labels = kmeanse.labels_\n",
    "    centers = kmeanse.cluster_centers_\n",
    "    for i in range(NC+1):\n",
    "        sse[NC] += np.sum((encoded[np.where(labels==i)]-centers[i])**2)\n",
    "        \n",
    "plt.plot(np.arange(NCr)+1,sse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NCr = 64\n",
    "sse = np.zeros(NCr)\n",
    "for NC in tqdm(range(NCr)): # number of clusters for K-means\n",
    "    kmeansb = KMeans(n_clusters=(NC+1)).fit(xviz)\n",
    "    labels = kmeansb.labels_\n",
    "    centers = kmeansb.cluster_centers_\n",
    "    for i in range(NC+1):\n",
    "        sse[NC] += np.sum((xviz[np.where(labels==i)]-centers[i])**2)\n",
    "        \n",
    "plt.plot(np.arange(NCr)+1,sse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "assistment_df = pd.read_csv('assistment_id.tsv', sep='\\t').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assistments=assistment_df.iloc[:,2:].values.astype(str) #Get rid of user_id\n",
    "user_list=assistment_df.iloc[:,0].values.astype(int) #Get user_id\n",
    "sentence_len=assistment_df.iloc[:,1].values.astype(int) #Extract number of responses\n",
    "sentences=[]\n",
    "#Copy only the ones that aren't nan\n",
    "for i in range(len(sentence_len)):\n",
    "    sentence=assistments[i,0:sentence_len[i]].tolist()\n",
    "    sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Each student is a \"sentence\", each skill is a \"word\"\n",
    "#size = dimensionality of feature vectors\n",
    "#window = max distance between current and predicted word within a sentence\n",
    "#min_count = minimum number of occurrences within dataset\n",
    "#workers = number of threads used\n",
    "#sg = 0 (CBOW, default); = 1 (skip-gram)\n",
    "model = Word2Vec(sentences, size=64, window=10, min_count=5, workers=4, sg=1, iter=30)\n",
    "\n",
    "assist_num=model.wv.vocab; #Names of the words (numbers)\n",
    "assist_vec=model[assist_num] #Access the vectors\n",
    "print(len(assist_num))\n",
    "print(assist_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of skills\n",
    "\n",
    "# Load entire Assistment dataframe to find the skills associated with each ID\n",
    "filename = 'skill_builder_data_corrected.csv'\n",
    "df = pd.read_csv(filename, encoding='ISO-8859-1', low_memory=False)\n",
    "df = df[(df['original'] == 1) & (df['attempt_count'] == 1) & ~(df['skill_name'].isnull())]\n",
    "\n",
    "print('Finding associated skills...')\n",
    "assist_skill=list()\n",
    "i = 0\n",
    "x = assist_vec\n",
    "print(x.shape)\n",
    "for k,v in tqdm(assist_num.items()):\n",
    "    if isinstance(k, str) & (k != 'nan'):\n",
    "        skill = df[df['assistment_id'] == int(float(k))]['skill_name'].iloc[0] # get the first skill associated with the assistment\n",
    "        assist_skill.append(skill)\n",
    "    else:\n",
    "        print(i)\n",
    "        x = np.delete(assist_vec, (i), axis=0)\n",
    "    i += 1\n",
    "    \n",
    "assist_vec = x\n",
    "assist_skill=np.asarray(assist_skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save results\n",
    "print(assist_skill.shape)\n",
    "print(len(assist_num))\n",
    "print(assist_vec.shape)\n",
    "print(x.shape)\n",
    "\n",
    "np.savez('../../data/word2vec_output.npz', assist_skill=assist_skill, assist_vec=assist_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look for clustering elbow\n",
    "\n",
    "NCr = 64\n",
    "sse = np.zeros(NCr)\n",
    "for NC in tqdm(range(NCr)): # number of clusters for K-means\n",
    "    kmeanse = KMeans(n_clusters=(NC+1)).fit(assist_vec)\n",
    "    labels = kmeanse.labels_\n",
    "    centers = kmeanse.cluster_centers_\n",
    "    for i in range(NC+1):\n",
    "        sse[NC] += np.sum((assist_vec[np.where(labels==i)]-centers[i])**2)\n",
    "        \n",
    "plt.plot(np.arange(NCr)+1,sse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NC = 32\n",
    "kmeanswv = KMeans(n_clusters=NC).fit(assist_vec)\n",
    "labelswv = kmeanswv.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(pd.Series(np.asarray(assist_skill)[np.where(labelswv==14)]).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unique skills\n",
    "skills_u = pd.Series(assist_skill).unique()\n",
    "print(skills_u.shape)\n",
    "\n",
    "# Which skills are in each cluster\n",
    "W = np.zeros((skills_u.shape[0], NC))\n",
    "\n",
    "\n",
    "# how many of each skill are in each cluster\n",
    "W1 = np.zeros((skills_u.shape[0], NC))\n",
    "\n",
    "\n",
    "for i in range(NC):\n",
    "    unqwv = pd.Series(assist_skill[np.where(labelswv==i)]).unique()\n",
    "    for skill in unqwv:\n",
    "        W[np.where(skills_u==skill), i] += 1;\n",
    "        W1[np.where(skills_u==skill), i] = pd.Series(assist_skill[np.where(labelswv==i)]).value_counts()[skill]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(W1)\n",
    "plt.figure()\n",
    "plt.plot(np.sum(W1,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelswv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
