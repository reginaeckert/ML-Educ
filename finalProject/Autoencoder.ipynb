{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE \n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### LOAD DATA INSTEAD OF RUNNING THIS #############################################\n",
    "# Create \"body_skill.csv\": csv containging unique, non-nan bodies and associated skill\n",
    "\n",
    "# load pre-selected dataset\n",
    "df3 = pd.read_csv('../../data/df3.csv')\n",
    "\n",
    "# get unique problem text\n",
    "body = df3['body'].unique()\n",
    "body.shape\n",
    "\n",
    "# Confirm that, yes, some problems have the same text\n",
    "dfsub = df3[['problem_id','body']].drop_duplicates(subset='problem_id')\n",
    "dfbody = dfsub.groupby('body').count()\n",
    "# dfbody is how many different problem ids each body is associated with\n",
    "\n",
    "# Begin processing to get skills for each (I know this is inefficeint...)\n",
    "body_small = body\n",
    "\n",
    "body_skill = [] # list of all skills, in orders\n",
    "body_edited = []\n",
    "for problem in tqdm(body_small):\n",
    "    if isinstance(problem, str):\n",
    "        body_edited.append(problem)\n",
    "        skill = df3[df3['body'] == problem]['skill_name'].iloc[0] # get the first skill associated with the assistment\n",
    "        body_skill.append(skill)\n",
    "        \n",
    "# save above\n",
    "body_save=pd.DataFrame({'body': body_edited,\n",
    "  'skill_name': body_skill})\n",
    "body_save.to_csv('../../data/body_skill_v2.csv',sep='\\t',index=False,columns=['body','skill_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### LOAD DATA ########\n",
    "df = pd.read_csv('../../data/body_skill.csv', sep='\\t')\n",
    "body = df['body'].values\n",
    "skills = df['skill_name'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "body_small = body\n",
    "print(type(body_small))\n",
    "body_small.shape\n",
    "\n",
    "def removeHTML(x):\n",
    "    # Remove not useful characters and HTML text\n",
    "    x = re.sub('<.*?>', '', x)\n",
    "    x = re.sub('\\n', ' ', x)\n",
    "    x = re.sub('&nbsp;', '', x)\n",
    "    x = re.sub('\\?', '', x)\n",
    "    x = re.sub('\\\\.(\\\\s)', ' ', x)\n",
    "    x = re.sub(',', '', x)\n",
    "    x = re.sub(':', '', x)\n",
    "    x = re.sub(';', ' ', x)\n",
    "    x = re.sub('\\(', '', x)\n",
    "    x = re.sub('\\)', '', x)\n",
    "    return x\n",
    "\n",
    "# Make list of lists of words in each problem\n",
    "problems = []   # list of lists of words in each problem\n",
    "wordsAll = []   # list of all words (duplicate words listed more than once)\n",
    "wordsPerProblem = Counter() # count of how many different problems each word occurred in\n",
    "for problem in tqdm(body_small):\n",
    "    if isinstance(problem, str):\n",
    "        text = removeHTML(problem.lower()) # remove HTML text\n",
    "        words = text.split() # split into words at white space\n",
    "        wordsPerProblem += Counter(set(words))\n",
    "        wordsAll += words\n",
    "        problems.append(words)\n",
    "        #print(Counter(set(words)).most_common(), '\\n\\n')\n",
    "    \n",
    "print(len(wordsAll))\n",
    "print(wordsPerProblem.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at the most common words in the dataset\n",
    "commonWords = list(np.array(Counter(wordsAll).most_common(50))[:,0])\n",
    "print(commonWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dictionary based on highedt average TFIDF\n",
    "N = len(problems)\n",
    "print('Total Number of problems: ',N)\n",
    "\n",
    "vs = 512 # vocab size\n",
    "dWordsAll = dict(Counter(wordsAll))\n",
    "dWordsProb = dict(wordsPerProblem)\n",
    "dDivide = {k: dWordsAll[k]/N*np.log(N/dWordsProb[k]) for k in dWordsProb.keys() & dWordsAll }\n",
    "c = Counter(dDivide)\n",
    "wordDict = list(np.array(c.most_common(vs))[:,0])\n",
    "print(wordDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "# tf_ij: number of times word i appears in problem j\n",
    "# df_i: number of problems containing word i\n",
    "# N: number of problems\n",
    "\n",
    "problems_small = problems\n",
    "N = float(len(problems))\n",
    "print('Number of problems: ',N)\n",
    "\n",
    "tfidf = []\n",
    "for problem in tqdm(problems_small):\n",
    "    c = Counter(problem)\n",
    "    tfidf_row = []\n",
    "    for word in wordDict:\n",
    "        tfidf_row.append(c[word]*np.log(N/wordsPerProblem[word]))\n",
    "    tfidf.append(np.array(tfidf_row))\n",
    "    \n",
    "tfidf = np.array(tfidf)\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.imshow(tfidf[::50,:])\n",
    "plt.colorbar()\n",
    "plt.clim((0,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run tsne\n",
    "tsne=TSNE(perplexity=30,verbose=2) #Instantiate the TSNE model (can change params here)\n",
    "tfidf_tsne=tsne.fit_transform(xviz) #Run tsne\n",
    "\n",
    "tsne_save=pd.DataFrame({'x': tfidf_tsne[:,0],\n",
    "  'y': tfidf_tsne[:,1],\n",
    "  'skill' : skills_viz})\n",
    "tsne_save.to_csv('../d3-scatterplot/tsne_body_bagofwords.tsv',sep='\\t',index=False,columns=['x','y','skill'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_tsne.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split bag of words data into training and testing set\n",
    "xtrain = []\n",
    "xtest =[]\n",
    "skills_train = []\n",
    "skills_test = []\n",
    "for i in np.arange(tfidf.shape[0]):\n",
    "    if np.mod(i,10) == 0:\n",
    "        xtest.append(tfidf[i,:])\n",
    "        skills_test.append(skills[i])\n",
    "    else:\n",
    "        xtrain.append(tfidf[i,:])\n",
    "        skills_train.append(skills[i])\n",
    "        \n",
    "xtrain = np.stack(xtrain, axis=0)\n",
    "xtest = np.stack(xtest, axis=0)\n",
    "skills_train = np.stack(skills_train, axis=0)\n",
    "skills_test = np.stack(skills_test, axis=0)\n",
    "print(xtrain.shape)\n",
    "print(xtest.shape)\n",
    "print(skills_train.shape)\n",
    "print(skills_test.shape)\n",
    "\n",
    "# Create visualization set (no nans)\n",
    "xviz = []\n",
    "skills_viz = []\n",
    "for i in np.arange(1,tfidf.shape[0],5):\n",
    "    if isinstance(skills[i],str):\n",
    "        xviz.append(tfidf[i,:])\n",
    "        skills_viz.append(skills[i])\n",
    "        \n",
    "xviz = np.stack(xviz, axis=0)\n",
    "skills_viz = np.stack(skills_viz, axis=0)\n",
    "\n",
    "plt.hist(np.ndarray.flatten(tfidf), bins=50)\n",
    "plt.ylim([0, 100000])\n",
    "\n",
    "norm = 20 # normalization\n",
    "xtrain = xtrain/norm\n",
    "xtest = xtest/norm\n",
    "xviz = xviz/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "#encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "input_img = Input(shape=(vs,))\n",
    "encoded = Dense(128, activation='relu')(input_img)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(vs, activation='sigmoid')(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(32,))\n",
    "# retrieve the last layers of the autoencoder model\n",
    "decoder_layers = autoencoder.layers[-3](encoded_input)\n",
    "decoder_layers = autoencoder.layers[-2](decoder_layers)\n",
    "decoder_layers = autoencoder.layers[-1](decoder_layers)\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layers)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(xtrain, xtrain,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(xtest, xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded = encoder.predict(xviz)\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test decoder\n",
    "auto = autoencoder.predict(xviz)\n",
    "decoded = decoder.predict(encoded)\n",
    "print(np.max(np.abs(auto[1,:]-decoded[1,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne=TSNE(perplexity=30,verbose=2) #Instantiate the TSNE model (can change params here)\n",
    "tfidf_tsne=tsne.fit_transform(encoded) #Run tsne\n",
    "tsne_save=pd.DataFrame({'x': tfidf_tsne[:,0],\n",
    "  'y': tfidf_tsne[:,1],\n",
    "  'skill' : skills_viz})\n",
    "#tsne_save=pd.DataFrame({'x': encoded[:,0],\n",
    "#  'y': encoded[:,1],\n",
    "#  'skill' : skills_viz})\n",
    "tsne_save.to_csv('../d3-scatterplot/tsne_bagofwords_autoencoder.tsv',sep='\\t',index=False,columns=['x','y','skill'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xpred = autoencoder.predict(xviz)\n",
    "plt.figure(figsize=(14,8))\n",
    "n = 1000\n",
    "plt.plot(xviz[n,:])\n",
    "plt.plot(xpred[n,:], '--')\n",
    "#print(xpred[n,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "NC = 32 # number of clusters for K-means\n",
    "kmeanse = KMeans(n_clusters=NC).fit(encoded)\n",
    "kmeansb = KMeans(n_clusters=NC).fit(xviz) #bag of words encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelse = kmeanse.labels_\n",
    "labelsb = kmeansb.labels_\n",
    "print(pd.Series(skills_viz[np.where(labelse==7)]).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(skills_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unique skills\n",
    "skills_u = pd.Series(skills_viz).unique()\n",
    "print(skills_u.shape)\n",
    "\n",
    "# Which skills are in each cluster\n",
    "A = np.zeros((skills_u.shape[0], NC))\n",
    "B = np.zeros((skills_u.shape[0], NC))\n",
    "\n",
    "# how many of each skill are in each cluster\n",
    "A1 = np.zeros((skills_u.shape[0], NC))\n",
    "B1 = np.zeros((skills_u.shape[0], NC))\n",
    "\n",
    "for i in range(NC):\n",
    "    unqe = pd.Series(skills_viz[np.where(labelse==i)]).unique()\n",
    "    unqb = pd.Series(skills_viz[np.where(labelsb==i)]).unique()\n",
    "    for skill in unqe:\n",
    "        A[np.where(skills_u==skill), i] += 1;\n",
    "        A1[np.where(skills_u==skill), i] = pd.Series(skills_viz[np.where(labelse==i)]).value_counts()[skill]\n",
    "    for skill in unqb:\n",
    "        B[np.where(skills_u==skill), i] += 1;\n",
    "        B1[np.where(skills_u==skill), i] = pd.Series(skills_viz[np.where(labelsb==i)]).value_counts()[skill]\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(skills_viz[np.where(labelse==80)]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(A1)\n",
    "plt.xlabel('cluster')\n",
    "plt.ylabel('skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(B1)\n",
    "plt.xlabel('cluster')\n",
    "plt.ylabel('skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.sum(A, axis=1))\n",
    "plt.xlabel('Skill')\n",
    "plt.figure()\n",
    "plt.plot(np.sum(A, axis=0))\n",
    "plt.xlabel('Cluster')\n",
    "plt.figure()\n",
    "plt.plot(np.sum(A1, axis=0))\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of things per cluster')\n",
    "print('Each skill is in an average of', np.mean(np.sum(A, axis=1)), 'clusters')\n",
    "print('Each cluster has an average of', np.mean(np.sum(A, axis=0)), 'skills')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.sum(B, axis=1))\n",
    "plt.xlabel('Skill')\n",
    "plt.figure()\n",
    "plt.plot(np.sum(B, axis=0))\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of skills per cluster')\n",
    "plt.figure()\n",
    "plt.plot(np.sum(B1, axis=0))\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of problems per cluster')\n",
    "print('Each skill is in an average of', np.mean(np.sum(B, axis=1)), 'clusters')\n",
    "print('Each cluster has an average of', np.mean(np.sum(B, axis=0)), 'skills')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xviz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of', 'to', 'a', 'in', 'and', 'for', 'the', 'is', 'that', 'what', 'below', 'by', 'value', 'if', 'find', 'are', 'fraction', 'percent', 'one', 'first', 'at', 'nearest', 'his', 'round', 'there', 'total', 'all', 'into', 'were', 'convert', '32', '52', '100', '28', 'new', 'dogs', '60', '31', '48', 'minivans']\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the cluster centers\n",
    "centers = kmeanse.cluster_centers_ # returns centers x dimensionality of space\n",
    "decoded_centers = decoder.predict(centers)\n",
    "\n",
    "s = 30;\n",
    "\n",
    "c = decoded_centers[s,:]\n",
    "b = np.zeros(c.shape)\n",
    "b = np.where(c>np.mean(c)*.5)\n",
    "w = [wordDict[i] for i in np.ndarray.tolist(b[0])]\n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Percents                            31\n",
      "Conversion of Fraction Decimals Percents    30\n",
      "Multiplication Fractions                     4\n",
      "Calculations with Similar Figures            2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(skills_viz[np.where(labelse==s)]).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
