{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read in skill builder dataset\n",
    "filename = 'skill_builder_data_corrected.csv'\n",
    "df = pd.read_csv(filename, encoding='ISO-8859-1', low_memory=False)\n",
    "df = df[(df['original'] == 1) & (df['attempt_count'] == 1) & ~(df['skill_name'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read in problem text dataset\n",
    "filename2='../data/problems.csv'\n",
    "problems=pd.read_csv(filename2, encoding='ISO-8859-1', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select students that have attempted more than n problems \n",
    "students_list=df.groupby('user_id').problem_id.count()\n",
    "students_id=students_list[students_list>50].index #Get the associated user_id\n",
    "df2=df[df['user_id'].isin(students_id)] #Select only the rows containing those students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Merge the two datasets based on the problem_id and assistment_id\n",
    "#Each assistment_id can have multiple problem_id's\n",
    "#But each problem_id appears to only be associated with 1 assistment_id\n",
    "df3=pd.merge(df2,problems,on=['assistment_id','problem_id'],how='left',indicator=True)\n",
    "#Will still include rows where there is no corresponding problem description for the problem_id\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Different way to merge\n",
    "df4=pd.merge(df2,problems,on=['assistment_id','problem_id'],how='inner')\n",
    "#Will cut out rows where there is no corresponding problem description for the problem_id\n",
    "#(This amounts to 120 rows and 23 unique problem_id's.)\n",
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Count the ones without problem text\n",
    "nondescript=df3[df3['_merge']=='left_only']['problem_id']\n",
    "print('Number of rows without description: ', nondescript.size)\n",
    "print('Number of unique problems without description: ', nondescript.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make sure each problem_id is only associated with 1 unique assistment_id\n",
    "any(df3.groupby('problem_id').assistment_id.nunique()>1)\n",
    "#Woo! No problem is associated with more than one assistment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Random queries\n",
    "df[df['problem_id']==58551]['assistment_id'].nunique()\n",
    "df[df['assistment_id']==76958]['problem_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Number of unique labeled skills (107)\n",
    "df3.skill_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "problems.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupby(['user_id','problem_id'])['skill_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[(df.problem_id==57647) & (df.user_id==14)]['position']\n",
    "#Multiple rows for a particular problem with multiple skills are the same (except for the skill info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['original']==1].problem_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupby(['user_id','problem_id']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.problem_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response_df = pd.read_csv('correct.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "skill_df = pd.read_csv('skill.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "assistment_df = pd.read_csv('assistment_id.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "skill_dict = {}\n",
    "with open('skill_dict.json', 'r', encoding='utf-8') as f:\n",
    "    loaded = json.load(f)\n",
    "    for k, v in loaded.items():\n",
    "        skill_dict[k] = int(v)\n",
    "\n",
    "skill_num = len(skill_dict) + 1 # including 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(skill_matrix, vocab_size):\n",
    "    '''\n",
    "    params:\n",
    "        skill_matrix: 2-D matrix (student, skills)\n",
    "        vocal_size: size of the vocabulary\n",
    "    returns:\n",
    "        a ndarray with a shape like (student, sequence_len, vocab_size)\n",
    "    '''\n",
    "    seq_len = skill_matrix.shape[1] #Number of exercises (sequence length)\n",
    "    #Initialize result (student, sequence, one-hot skill) to zeros\n",
    "    result = np.zeros((skill_matrix.shape[0], seq_len, vocab_size)) \n",
    "    #For each student\n",
    "    for i in range(skill_matrix.shape[0]):\n",
    "        #Select the student, all sequences, and the related skill; set to 1\n",
    "        result[i, np.arange(seq_len), skill_matrix[i]] = 1.\n",
    "    return result\n",
    "\n",
    "def dkt_one_hot(skill_matrix, response_matrix, vocab_size):\n",
    "    #Number of exercises/skills\n",
    "    seq_len = skill_matrix.shape[1]\n",
    "    #Initialize output (student, sequence, 2 * vocab size) to zeros\n",
    "    skill_response_array = np.zeros((skill_matrix.shape[0], seq_len, 2 * vocab_size))\n",
    "    #For each student\n",
    "    for i in range(skill_matrix.shape[0]):\n",
    "        #Set to 1 the (student, all sequences, skill location + [0 1] if correct and + [1 0] if incorrect)\n",
    "        skill_response_array[i, np.arange(seq_len), 2 * skill_matrix[i] + response_matrix[i]] = 1.\n",
    "    return skill_response_array\n",
    "#Function to preprocess the data\n",
    "def preprocess(skill_df, response_df, skill_num):\n",
    "    skill_matrix = skill_df.iloc[:, 1:].values #Select values (excluding first column, which is index)\n",
    "    response_array = response_df.iloc[:, 1:].values\n",
    "    #Get the one-hots associated with each (student, sequence, skill one-hot)\n",
    "    skill_array = one_hot(skill_matrix, skill_num)\n",
    "    #Get the one-hots associated with (student, sequence, response one-hot)\n",
    "    #and (student, sequence, skill one-hot)\n",
    "    #skill_response_array, masking_array = dkt_one_hot(skill_matrix, response_array, skill_num)\n",
    "    skill_response_array = dkt_one_hot(skill_matrix, response_array, skill_num)\n",
    "    return skill_array, response_array, skill_response_array, masking_array\n",
    "    \n",
    "\n",
    "skill_array, response_array, skill_response_array = preprocess(skill_df, response_df, skill_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, LSTM, TimeDistributed, Lambda, multiply\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "\n",
    "#Function to set up skill to skill model (input skills, output skill prediction)\n",
    "def build_skill2skill_model(input_shape, lstm_dim=32, dropout=0.0):\n",
    "    input = Input(shape=input_shape, name='input skills')\n",
    "    lstm = LSTM(lstm_dim, \n",
    "                return_sequences=True, \n",
    "                dropout=dropout,\n",
    "                name='lstm layer')(input)\n",
    "    output = TimeDistributed(Dense(input_shape[-1], activation='softmax'), name='probability')(lstm)\n",
    "    model = Model(inputs=[input], outputs=[output])\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def reduce_dim(x):\n",
    "    x = K.max(x, axis=-1, keepdims=True)\n",
    "    return x\n",
    "\n",
    "#Skill and response\n",
    "def build_dkt_model(input_shape, lstm_dim=32, dropout=0.0):\n",
    "    input_skills = Input(shape=input_shape, name='input_skills')\n",
    "    lstm = LSTM(lstm_dim, \n",
    "                return_sequences=True, \n",
    "                dropout=dropout,\n",
    "                name='lstm_layer')(input_skills)\n",
    "    dense = TimeDistributed(Dense(int(input_shape[-1]/2), activation='sigmoid'), name='probability for each')(lstm)\n",
    "    \n",
    "    skill_next = Input(shape=(input_shape[0], int(input_shape[1]/2)), name='next_skill_tested')\n",
    "    merged = multiply([dense, skill_next], name='multiply')\n",
    "    reduced = Lambda(reduce_dim, output_shape=(input_shape[0], 1), name='reduce dim')(merged)\n",
    "    \n",
    "    model = Model(inputs=[input_skills, skill_next], outputs=[reduced])\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "print('skill2skill')\n",
    "skill2skill_model = build_skill2skill_model((99, skill_num), lstm_dim=64)\n",
    "\n",
    "print('dkt')\n",
    "dkt_model = build_dkt_model((99, 2 * skill_num), lstm_dim=64)\n",
    "\n",
    "# train skill2skill\n",
    "skill2skill_model.fit(skill_array[:, 0:-1], \n",
    "                      skill_array[:, 1:],\n",
    "                      epochs=20, \n",
    "                      batch_size=32, \n",
    "                      shuffle=True,\n",
    "                      validation_split=0.2)\n",
    "\n",
    "dkt_model.fit([skill_response_array[:, 0:-1], skill_response_array[:, 1:]],\n",
    "              response_array[:, 1:, np.newaxis],\n",
    "              epochs=20, \n",
    "              batch_size=32, \n",
    "              shuffle=True,\n",
    "              validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Our code\n",
    "# modified lstm_dim in different trials\n",
    "skill2skill_model = build_skill2skill_model((99, skill_num), lstm_dim=64)\n",
    "\n",
    "# added early stopping + increased number of epochs for later trials\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "skill2skill_model.fit(skill_array_train[:, 0:-1], \n",
    "                      skill_array_train[:, 1:],\n",
    "                      epochs=20, \n",
    "                      batch_size=32, \n",
    "                      shuffle=True,\n",
    "                      validation_data=validation_set,\n",
    "                      callbacks=[early_stopping])\n",
    "\n",
    "# check accuracy of model on test set\n",
    "testx = skill2skill_model.predict(skill_array_test[:, 0:-1])\n",
    "testy = skill_array_test[:,1:];\n",
    "\n",
    "skill_predicted = np.argmax(testx,axis=2) # which skill was predicted\n",
    "skill_gndtruth = np.argmax(testy, axis=2) # which skill was really next\n",
    "pred_acc = (skill_predicted==skill_gndtruth) # if the prediction was correct\n",
    "\n",
    "print('Accuracy: ', np.mean(pred_acc))\n",
    "\n",
    "# percent correct by skill\n",
    "\n",
    "testx = skill2skill_model.predict(skill_array_test[:, 0:-1])\n",
    "testy = skill_array_test[:,1:];\n",
    "\n",
    "skill_predicted = np.argmax(testx,axis=2) # which skill was predicted\n",
    "skill_gndtruth = np.argmax(testy, axis=2) # which skill was really next\n",
    "pred_acc = (skill_predicted==skill_gndtruth) # if the prediction was correct\n",
    "\n",
    "num_corr_skill = np.zeros(skill_num) # number of time skill was correctly predicted\n",
    "num_occurrence_skill = np.zeros(skill_num) # number of times the skill appeared\n",
    "for i in np.arange(testx.shape[0]):\n",
    "    for j in np.arange(testx.shape[1]):\n",
    "        num_occurrence_skill[skill_gndtruth[i,j]] += 1;\n",
    "        num_corr_skill[skill_gndtruth[i,j]] += pred_acc[i,j]\n",
    "\n",
    "skill_acc = num_corr_skill/num_occurrence_skill\n",
    "\n",
    "plt.plot(num_occurrence_skill, skill_acc, 'o')\n",
    "plt.xlabel('Number of times skill occurred')\n",
    "plt.ylabel('Prediction Accurracy')\n",
    "plt.grid('on')\n",
    "\n",
    "# inverse dictionart to find skill names from values\n",
    "skill_dict_inv = {v: k for k, v in skill_dict.items()}\n",
    "\n",
    "k = 5 # top 5 easiest/hardest to predict\n",
    "\n",
    "# easiest to predict skills\n",
    "num_nans = np.sum(np.isnan(skill_acc));\n",
    "top_skill_index = np.flip(skill_acc.argsort()[(-num_nans-k):(-num_nans)], axis=0)\n",
    "print('Easiest to predict')\n",
    "print('Index: ', top_skill_index)\n",
    "print('Accuracy: ', skill_acc[top_skill_index])\n",
    "print('Num Occurrences: ', num_occurrence_skill[top_skill_index])\n",
    "for i in np.arange(k):\n",
    "    print(skill_dict_inv[top_skill_index[i]])\n",
    "\n",
    "# hardest to predict skills\n",
    "# disregard skills with fewer than 50 occurrences in test set\n",
    "skill_acc_modified = skill_acc\n",
    "skill_acc_modified[num_occurrence_skill < 50] = np.nan\n",
    "bottom_skill_index = skill_acc.argsort()[:k]\n",
    "print('\\nHardest to predict')\n",
    "print('Index: ',bottom_skill_index)\n",
    "print('Accuracy: ', skill_acc[bottom_skill_index])\n",
    "print('Num Occurrences: ',num_occurrence_skill[bottom_skill_index])\n",
    "for i in np.arange(k):\n",
    "    print(skill_dict_inv[bottom_skill_index[i]])\n",
    "  \n",
    "\n",
    "validation_set_dkt=([skill_response_array[testing_mask, 0:-1], skill_array[testing_mask, 1:]], response_array[testing_mask, 1:, np.newaxis] );\n",
    "dkt_model.fit([skill_response_array[training_mask, 0:-1], skill_array[training_mask, 1:]],\n",
    "              response_array[training_mask, 1:, np.newaxis],\n",
    "              epochs=20, \n",
    "              batch_size=32, \n",
    "              shuffle=True,\n",
    "              validation_data=validation_set_dkt)\n",
    "\n",
    "    import sklearn as sklearn\n",
    "#Predicted values\n",
    "response_predict=dkt_model.predict([skill_response_array[testing_mask, 0:-1], skill_array[testing_mask, 1:]])\n",
    "#Actual values\n",
    "response_actual=response_array[testing_mask,1:]\n",
    "\n",
    "#Find AUC\n",
    "score=sklearn.metrics.roc_auc_score(np.reshape(response_actual,(-1,1)),np.reshape(response_predict,(-1,1)))\n",
    "print(score)\n",
    "\n",
    "#Build model\n",
    "dkt_model = build_dkt_model((99, 2 * skill_num), lstm_dim=128)\n",
    "\n",
    "#Train model\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "dkt_model.fit([skill_response_array[training_mask, 0:-1], skill_array[training_mask, 1:]],\n",
    "              response_array[training_mask, 1:, np.newaxis],\n",
    "              epochs=50, \n",
    "              batch_size=32, \n",
    "              shuffle=True,\n",
    "              validation_data=validation_set_dkt)#,\n",
    "              #callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Skills\n",
    "#Load in skills dataframe\n",
    "skill_df = pd.read_csv('skill.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "#Load in dictionary associating skill numbers with skill names\n",
    "skill_dict = {}\n",
    "with open('skill_dict.json', 'r', encoding='utf-8') as f:\n",
    "    loaded = json.load(f)\n",
    "    for v, k in loaded.items():\n",
    "        skill_dict[k] = str(v) #Use number as key, string as value\n",
    "\n",
    "        #Read out the \"sentences\"\n",
    "sentences=skill_df.iloc[:,1:].values.astype(str)\n",
    "sentences=sentences.tolist()\n",
    "\n",
    "#Each student is a \"sentence\", each skill is a \"word\"\n",
    "#size = dimensionality of feature vectors\n",
    "#window = max distance between current and predicted word within a sentence\n",
    "#min_count = minimum number of occurrences within dataset\n",
    "#workers = number of threads used\n",
    "#sg = 0 (CBOW, default); = 1 (skip-gram)\n",
    "model = Word2Vec(sentences, size=200, window=10, min_count=10, workers=4, sg=1, iter=100)\n",
    "\n",
    "skill_num=model.wv.vocab; #Names of the words (numbers)\n",
    "skill_vec=model[skill_num] #Access the vectors\n",
    "\n",
    "skill_name=list()\n",
    "#Associate with readable words\n",
    "for k,v in skill_num.items(): #Iterate over the vocab from word2vec (k = key = number string)\n",
    "    skill_name.append(skill_dict.get(k)) #Get the value (tag) saved at that key in the other dict\n",
    "#print(skill_name)\n",
    "\n",
    "tsne=TSNE(perplexity=30) #Instantiate the TSNE model (can change params here)\n",
    "skill_tsne=tsne.fit_transform(skill_vec.astype(float)) #Run tsne\n",
    "\n",
    "#Save as a tsv file for d3-scatterplot\n",
    "# d={'x': skill_tsne[:,0],\n",
    "#   'y': skill_tsne[:,1],\n",
    "#   'skill' : skill_name}\n",
    "tsne_save=pd.DataFrame({'x': skill_tsne[:,0],\n",
    "  'y': skill_tsne[:,1],\n",
    "  'skill' : skill_name})\n",
    "tsne_save.to_csv('../d3-scatterplot/tsne_skills.tsv',sep='\\t',index=False,columns=['x','y','skill'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assistments\n",
    "# Load in Assistments ID dataframe\n",
    "assistment_df = pd.read_csv('assistment_id.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "sentences=assistment_df.iloc[:,1:].values.astype(str)\n",
    "sentences=sentences.tolist()\n",
    "\n",
    "# Load entire Assistment dataframe to find the skills associated with each ID\n",
    "filename = 'skill_builder_data_corrected.csv'\n",
    "df = pd.read_csv(filename, encoding='ISO-8859-1', low_memory=False)\n",
    "df = df[(df['original'] == 1) & (df['attempt_count'] == 1) & ~(df['skill_name'].isnull())]\n",
    "\n",
    "#Each student is a \"sentence\", each skill is a \"word\"\n",
    "#size = dimensionality of feature vectors\n",
    "#window = max distance between current and predicted word within a sentence\n",
    "#min_count = minimum number of occurrences within dataset\n",
    "#workers = number of threads used\n",
    "#sg = 0 (CBOW, default); = 1 (skip-gram)\n",
    "model = Word2Vec(sentences, size=200, window=10, min_count=10, workers=4, sg=1, iter=30)\n",
    "\n",
    "assist_num=model.wv.vocab; #Names of the words (numbers)\n",
    "assist_vec=model[assist_num] #Access the vectors\n",
    "\n",
    "tsne=TSNE(perplexity=30) #Instantiate the TSNE model (can change params here)\n",
    "assist_tsne=tsne.fit_transform(assist_vec.astype(float)) #Run tsne\n",
    "\n",
    "assist_skill=list()\n",
    "\n",
    "for k,v in assist_num.items():\n",
    "    skill = df[df['assistment_id'] == int(k)]['skill_name'].iloc[0] # get the first skill associated with the assistment\n",
    "    assist_skill.append(skill)\n",
    "\n",
    "    #Save as a tsv file for d3-scatterplot\n",
    "# d={'x': skill_tsne[:,0],\n",
    "#   'y': skill_tsne[:,1],\n",
    "#   'skill' : skill_name}\n",
    "tsne_save=pd.DataFrame({'x': assist_tsne[:,0],\n",
    "  'y': assist_tsne[:,1],\n",
    "  'skill' : assist_skill})\n",
    "tsne_save.to_csv('../d3-scatterplot/tsne_assist.tsv',sep='\\t',index=False,columns=['x','y','skill'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define\n",
    "def generate_datasets(df):\n",
    "    users_list = df['user_id'].unique() #List of unique user ids\n",
    "    skill_list = df['skill_name'].unique() #List of unique skills\n",
    "    \n",
    "    #Create skill dict object\n",
    "    skill_dict = dict(zip(skill_list, np.arange(len(skill_list), dtype='int32') + 1))\n",
    "    \n",
    "    #Initialize response, skill, and assistment list\n",
    "    response_list = []\n",
    "    skill_list = []\n",
    "    assistment_list = []\n",
    "    \n",
    "    counter = 0\n",
    "    #For each user\n",
    "    for user in users_list:\n",
    "        #Select all the data for that user\n",
    "        sub_df = df[df['user_id'] == user]\n",
    "        num_resp=len(sub_df)\n",
    "        #number of responses for that user\n",
    "        #If that user has more than 100 responses\n",
    "        if num_resp > 50:\n",
    "            #Select the first hundred responses, skills, and assistments\n",
    "            \n",
    "            first_hundred = sub_df.iloc[0:num_resp]\n",
    "            #Create the dataframe spaces to hold the data\n",
    "            response_df = pd.DataFrame(index=[counter], columns=['student_id']+['r'+str(i) for i in range(num_resp)])\n",
    "            skill_df = pd.DataFrame(index=[counter], columns=['student_id']+['s'+str(i) for i in range(num_resp)])\n",
    "            assistment_df = pd.DataFrame(index=[counter], columns=['student_id']+['a'+str(i) for i in range(num_resp)])\n",
    "            \n",
    "            #Copy over the user id info\n",
    "            response_df.iloc[0, 0] = first_hundred.iloc[0]['user_id']\n",
    "            skill_df.iloc[0, 0] = first_hundred.iloc[0]['user_id']\n",
    "            assistment_df.iloc[0, 0] = first_hundred.iloc[0]['user_id']\n",
    "            #fill in the responses, skills, and assistments\n",
    "            for i in range(num_resp):\n",
    "                response_df.iloc[0, i+1] = first_hundred.iloc[i]['correct']\n",
    "                skill_df.iloc[0, i+1] = skill_dict[first_hundred.iloc[i]['skill_name']]\n",
    "                assistment_df.iloc[0, i+1] = first_hundred.iloc[i]['assistment_id']\n",
    "            counter += 1\n",
    "            #Add to the overall list\n",
    "            response_list.append(response_df)\n",
    "            skill_list.append(skill_df)\n",
    "            assistment_list.append(assistment_df)\n",
    "    \n",
    "    #Convert to a dataframe\n",
    "    response_df = pd.concat(response_list)\n",
    "    skill_df = pd.concat(skill_list)\n",
    "    assistment_df = pd.concat(assistment_list)\n",
    "    \n",
    "    #Return\n",
    "    return skill_dict, response_df, skill_df, assistment_df\n",
    "    \n",
    "#Use function to generate the dataset as required\n",
    "skill_dict, response_df, skill_df, assistment_df = generate_datasets(df3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(assistment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save the data files\n",
    "with open('skill_dict.json', 'w', encoding='utf-8') as f:\n",
    "    to_dump_dict = {}\n",
    "    for key, value in skill_dict.items():\n",
    "        to_dump_dict[key] = str(value)\n",
    "    json.dump(to_dump_dict, f)\n",
    "response_df.to_csv('correct.tsv', sep='\\t')\n",
    "skill_df.to_csv('skill.tsv', sep='\\t')\n",
    "assistment_df.to_csv('assistment_id.tsv', sep='\\t')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assistment_df = pd.read_csv('assistment_id.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences=assistment_df.iloc[:,:-1].values.astype(str) #Get rid of user_id\n",
    "sentences=sentences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
